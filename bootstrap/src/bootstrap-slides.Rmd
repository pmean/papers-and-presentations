---
title: "A gentle introduction to the bootstrap"
author: "Steve Simon"
date: "Created 2022-07-13"
output: powerpoint_presentation
---

```{r setup, echo=FALSE}
knitr::opts_chunk$set(echo = FALSE)
suppressMessages(
  suppressWarnings(
    library(ggplot2)
  )
)
```

### Outline

+ History
  + The jackknife
  + Bradley Efron's work
  + Bagging
+ Purpose
  + Estimate bias
  + Calculate standard errors
  + Compute confidence intervals
  + Test hypotheses
+ Resampling mechanics
+ Calculations
  + Bias
  + Standard error
  + Percentile confidence interval
  + Bias corrected intervals
+ Software
  + SAS
  + Stata
  + R

<div class="notes">

Here is the abstract that I provided. I am including it here to remind myself what I promised to talk about.

The bootstrap is a methodology derived by Bradley Efron in the 1980s that provides a reasonable approximation to the sampling distribution of various "difficult" statistics. Difficult statistics are those where there is no mathematical theory to establish a distribution. It is also useful when you don't trust the mathematical theory because of a small sample size or potential violations of the underlying assumptions. The bootstrap is also a mechanism used by many machine learning algorithms to avoid overfitting. This talk will orient you to the general mechanisms of the bootstrap algorithm and illustrate its application in a couple of simple settings.

This talk will cover four major areas.

First, I will provide a historical overview, starting with a simpler method that the bootstrap was based on called the jackknife. Then I will talk about Bradlet Efron's work to develop the bootstrap and establish its theoretical foundations. Then I will mention how bootstrapping has developed into a methodology used in machine learning called bagging.

Next, I will explain the reasons why you might want to use the bootstrap: to estimate bias, calculate standard errors, compute confidence intervals, and test hypotheses.

I will illustrate the mechanics of the bootstrap and show briefly how to implement the bootstrap in SAS, Stata, and R.



+ Purpose
  + Estimate bias
  + Calculate standard errors
  + Compute confidence intervals
  + Test hypotheses
+ Calculations
  + Bias
  + Standard error
  + Percentile confidence interval
  + Bias corrected intervals
+ Software
  + SAS
  + Stata
  + R


</div>

### History of the bootstrap

+ Can you rely on asymptotic normality?
+ The jackknife
+ Bradley Efron's contributions
+ Recent application: bagging

<div class="notes">

I want to provide a bit of historical context. Before the bootstrap came along, researchers relied on a variety of mathematical theorems like the Central Limit Theorem and extensions to the Central Limit Theorem to estimate bias, calculate standard errors, produce confidence intervals, and test hypotheses.

The bootstrap represents an early attempt to use the power of computer simulation to estimate bias, calculate standard errors, produce confidence intervals, and test hypotheses. The bootstrap provides these answers in many settings where you can't find a variation on the Central Limit Theorem that would apply or when you don't trust the approximation. I'll provide a brief overview of the jackknife, an earlier approach that the bootstrap was based on. Then I'll talk about Bradley Efron's work in the 1970's and 1980's to develop the bootstrap and to establish the mathematical principles that make the bootstrap work in so many different areas. Finally, I will talk about how bootstrapping came to be relied on in various machine learning algorithms.

</div>

### Can we rely on asymptotic normality? (1/4)

![Figure 1. Cover of book by Robert Serfling](../images/serfling-book-cover.jpg)

<div class="notes">

I need to start with a book that I used when I was in graduate school. The title is "Approximation Theorems in Mathematical Statistics" by Rboery Serfling. It was all about the variety of ways to show that some statistic followed an asymptotic normal distribution.

</div>

### Can we rely on asymptotic normality? (2/4)

$\bar{X}=\frac{1}{n}\sum_{i=1}^{n} X_i$ is approximately normal if

+ The $X_i$ all come from the same distribution
+ The $X_i$'s are all independent
+ The $X_i$ have a finite second moment

A more precise statement

+ $lim_{n \to \infty}\  \frac{\bar{X}-\mu}{\sigma / \sqrt{n}} = N(0,1)$

<div class="notes">

I'm sure you're all familiar with the Central Limit Theorem. It states that the average of independent identically distributed random variables is approximately normal.

The rule of thumb is that you can trust the normal approximation when the sample size is greater than 30. There is a lot that you can quibble about with respect to the cut-off of 30, but we're not going to get too fussy about this.

</div>

### Can we rely on asymptotic normality? (3/4)

Furthermore,

$E[\bar{X}]=\mu$

$Var(\bar{X})=\frac{\sigma^2}{n}$

<div class="notes">

You can also show easily that the expected value of the sample mean is mu (the sample mean is an unbiased estimate of the population mean) and that the variance of the sample mean is the variance of an individual X value divided by the sample size n.

</div>

### Can we rely on asymptotic normality? (4/4)

What about: 

$MAD(X)=\frac{1}{n}\sum_{i=1}^{n}|X_i-\bar{X}|$ 

$\ $

$IQR = X_{.75}-X_{.25}$

$\ $

$n < 30$

<div class="notes">

What about more complex settings?

What if the sample size is not large enough to rely on the Central Limit Theorem?

What if you are measuring the mean absolute deviation (the average of the absolute values of each individual value minus the sample mean) or the interquartile range (the difference between the 75th percentile and the 25th percentile).

If you are really clever and if you understand all the approximation theorems in Robert Serfling's book, you will know how to establish an approximation to these statistics (usually a normal approximation, but sometimes there are other distributions like the chi-square distribution that represent a good approximation).

But an even more fundamental question is what do you do when the sample size is not large enough to justify the use of the Central Limit Theorem? I put down n<30 here, but in some settings (well behaved distributions without much skewness and only a weak tendency to produce outliers), you might get by with only 10 observations. Other times (extremely skewed distributions and/or a strong tendency to produce outliers), even a sample size of 300 is inadequate to assume an approximately normal distribution.

It turns out that you can use simulations involving the data itself to establish an underlying distribution.

</div>

### The jackknife (1/4)

![Figure 1. Image of a jackknife](../images/jackknife-image.png)

<div class="notes">

The jackknife was first developed in 1949 by Maurice Quenouille and was extended to a more general setting by John Tukey in the 1950s. Dr. Tukey was fond of giving clever names to various statistical terms. He was the one, for example, who coined the term "bit" as a shorted form of binary digit. He chose the name "jackknife" for the Quenouille approach because the jackknife is an all-purpose tool.

</div>

### The jackknife (2/4)

$X\ \ \ \ \ \  = (2,3,7,5,6)$

$\ $

$X^{(-1)} = (\ \ ,3,7,5,6)$

$X^{(-2)} = (2,\ \ ,7,5,6)$

$X^{(-3)} = (2,3,\ \ ,5,6)$

$X^{(-4)} = (2,3,7,\ \ ,6)$

$X^{(-5)} = (2,3,7,5,\ \ )$

<div class="notes">

The jackknife, as called the "leave-one-out" method was proposed in 1949 as a method for estimating bias and calculating standard errors by Quenouille. It got the name "jackknife" by John Tukey because he felt it was a useful tool for a variety of settings.

You create subsamples by leaving one data point out. With five data points, you have five subsamples.

</div>

### The jackknife (3/4)

```{r}
mad <- function(u) {mean(abs(u-mean(u)))}

x <- c(2,3,7,5,6)
m0 <- mad(x)
m1 <- mad(x[-1])
m2 <- mad(x[-2])
m3 <- mad(x[-3])
m4 <- mad(x[-4])
m5 <- mad(x[-5])
mn <- mean(c(m1, m2, m3, m4, m5))
sn <- sd(c(m1, m2, m3, m4, m5))
```

$MAD$ (`r paste(x, sep=",")`) = `r m0`

$\ $

$MAD$ (`r paste(x[-1], sep=",")`) = `r m1` 

$MAD$ (`r paste(x[-2], sep=",")`) = `r m2` 

$MAD$ (`r paste(x[-3], sep=",")`) = `r m3` 

$MAD$ (`r paste(x[-4], sep=",")`) = `r m4` 

$MAD$ (`r paste(x[-5], sep=",")`) = `r m5` 

<div class="notes">

### The jackknife (4/4)

+ MAD (Full sample) = `r m0`
+ Average MAD (Jackknife subsamples) = `r mn`
+ Standard deviation MAD (Jackknife subsamples) = `r round(sn, 3)`

### Bradley Efron's contribution (1/5)

![Figure 1. Photograph of Bradley Efron with President Bush](../images/bradley-efron-02.jpg)

<div class="notes">

This image is from a White House ceremony where Bradley Efron received the President's National Medal of Science. I was quite shocked when I found this picture a few days ago, and I've been trying to call Joe Biden ever since to see where my medal is. Seriously, you have to be a really special statistician to deserve an honor like this.

Most of the information about Bradley Efron comes from

Denise LaFontaine. The History of Bootstrapping: Tracing the Development of Resampling With Replacement. The Mathematics Enthusiast 2021, 18(1). Available in [pdf format][laf1].

[laf1]: https://scholarworks.umt.edu/cgi/viewcontent.cgi?article=1515&context=tme

Bradley Efron entered the PhD program in Statistics at Stanford University in 1960. He was influenced by one of the faculty at Stanford, Rupert Miller, who was working on establishing conditions under which the jackknife did or did not perform well. Shortly after graduating, Dr. Efron started working on an approach that would fix some of the shortcomings of the jackknife.

</div>

### Bradley Efron's contribution (2/5)

The bootstrap sample=sampling with replacement.

```{r}
set.seed(1245)
b1 <- matrix(sample(x, 2500, replace=TRUE), nrow=500)
m <- apply(b1, 1, mad)
mn <- mean(m)
sn <- sd(m)
```

Bootstrap sample #1: (`r paste(b1[1, ], sep=",")`)

Bootstrap sample #2: (`r paste(b1[2, ], sep=",")`)

Bootstrap sample #3: (`r paste(b1[3, ], sep=",")`)

...

Bootstrap sample #500: (`r paste(b1[500, ], sep=",")`)

### Bradley Efron's contribution (3/5)

MAD(`r paste(b1[1, ], sep=",")`) = `r m[1]`

MAD(`r paste(b1[2, ], sep=",")`) = `r m[2]`

MAD(`r paste(b1[3, ], sep=",")`) = `r m[3]`

...

MAD(`r paste(b1[500, ], sep=",")`) = `r m[500]`

### Bradley Efron's contribution (4/5)

+ MAD (Full sample) = `r m0`
+ Average MAD (Bootstrap samples) = `r round(mn, 3)`
+ Standard deviation MAD (Bootstrap samples) = `r round(sd(m), 3)`

### Bradley Efron's contribution (5/5)

```{r}
g1 <- ggplot(data.frame(m=m), aes(x=m)) + 
  geom_histogram(binwidth=0.1)
ggsave("../images/histogram01.png", g1, width=6, height=6, units = "in")
```

![Figure 1. Histogram of bootstrapped estimates](../images/histogram01.png)

<div class="notes">

This is a histogram of the 500 bootstrapped estimates of the mean absolute deviation. Notice that it is "patchy" and does not follow a smooth bell shaped curve. This is an important issue that we will address when computing confidence intervals.

</div>

### Bagging (1/4)

+ Portmanteau for bootstrap aggregation
  + Used in random forests
+ Developed by Leo Breiman in 1996
+ Start with CART model
  + Classification And Regression Tree

<div class="notes">

I want to briefly address an important application of the bootstrap to machine learning. Bagging is a portmanteau, a combination and shortening of two words: bootstrap aggregation.

The basic building block for bagging in the random forest model is the CART model, Classification and Regression Trees.

</div>

### Bagging (2/4)


![Figure 1. Illustration of a regression tree](../images/regression-tree.png)

<div class="notes">

A regression tree is a model used for continuous outcomes. It finds optimal splits of the data that create subgroups where the outcome variable shows very little variation. This is an example from the Statology blog on how to fit CART models.

Zach Bobbitt. How to Fit Classification and Regression Trees in R. Statology blog, 2020-11-22. Available in [html format][bobb1].

[bobb1]: https://www.statology.org/classification-and-regression-trees-in-r/

The graph shows a prediction model for baseball player salaries. If the numbers seem low, it is because the data comes form 1987. The first split is between years in the league. If it is less than 4.5, the node to the left shows a mean salary of 225.83 thousand dollars. If it is greater than 4.5, the node to the right shows an additional split: were the number of home runs less than 16.5, then another split is the number of home runs also less than 8.5 then the mean salary is 502.81 thousand dollars. I won't go through every branch, but each of the final nodes is a combination of splits involving years in the league or home runs.

</div>

### Bagging (3/4)

![Figure 1. Image of a classification tree](../images/classification-tree.jpg)

<div class="notes">

This is an example of a classification tree. You use a classification tree when you are predicting a binary outcome.

This is an example from the [Wikipedia page on decision tree learning][wik1]. It is work by Gilgoldm and published under a Creative Commons open source license (CC BY-SA 4.0) and is available for download [here][wik2].

[wik1]: https://en.wikipedia.org/wiki/Decision_tree_learning
[wik2]: https://commons.wikimedia.org/w/index.php?curid=90405437

Both classification trees and regression trees have a tendency to overfit the data. They are also highly sensitive to small changes in the data. In fact, I would have a hard time recommending the use of these models at all.

There is an approach, however, that largely overcomes these concerns. It is called an ensemble approach. You combine multiple regression or classification trees into a "forest." And you do this with the help of the bootstrap.

</div>

### Bagging (4/4)

Bootstrap sample b=1: CART model predictions, $\hat{Y}_{(1)}$

Bootstrap sample b=2: CART model predictions, $\hat{Y}_{(2)}$

Bootstrap sample b=3: CART model predictions, $\hat{Y}_{(3)}$

...

Bootstrap sample b=B: CART model predictions, $\hat{Y}_{(B)}$

Final prediction: $\frac{1}{B}\sum_{b=1}^B \ \hat{Y}_b$


<div class="notes">

In bagging, you fit a model (in the case of Random Forests, you fit a CART model) to a few hundred or thousand bootstrap samples. Get predicted values for each model. Average those predicted values across all the bootstrap samples.

There are some additional enhancements to the Random Forest models, but the key element is the bagging step.

Note: Each bootstrap sample might produce a different set of independent variables, so you can't say anything directly about which variables help the most in predicting the outcome. You can't get p-values or confidence intervals for individual independent variables. There are some indirect ways to assess this, but I will not talk about these.

</div>

### Break #1

+ What have you learned
  + History of the bootstrap
+ What's coming next
  + Reasons for using the bootstrap
  
### Reasons for using the bootstrap

+ Estimate bias
+ Calculate standard errors
+ Compute confidence intervals
+ Test hypotheses

### Estimate bias

+ $\hat\theta=\hat\theta(X_1, X_2,..., X_n)$ is an estimate of $\theta$.
+ Recalculate for B bootstrap samples
  + $\hat\theta^{(1)}=\hat\theta(X^(1)_1,X^(1)_2,...,X^(1)_n)$
  + $\hat\theta^{(2)}=\hat\theta(X^(2)_1,X^(2)_2,...,X^(2)_n)$
  + ...
  + $\hat\theta^{(B)}=\hat\theta(X^(B)_1,X^(B)_2,...,X^(B)_n)$
+ Compare the bootstrap average to the original estimate
  + $\frac{1}{B}\sum_{b=1}^{B}\hat\theta^{(b)}-\hat\theta$

### Calculate standard errors

### Compute confidence intervals

### Test hypotheses

### Break #2

+ What you have learned
  + Purposes of the bootstrap
+ What's coming next
  + Mechanics
  
### Mechanics

+ Resampling
+ Bootstrap estimates
+ Bias calculation
+ Standard error calculation
+ Confidence interval
  + Percentile method
  + Bias corrected and adjusted method
+ Hypothesis tests

### Break #3

+ What have you learned
  + Mechanics
+ What's coming next
  + Software
  
### Software

### Special issues

+ Time series
+ Regression models

### Summary

+ History
+ Purpose
+ Calculations
+ Software