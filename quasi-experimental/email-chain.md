---
title: "Email chain"
format: html
editor: source
---

## 2023-12-13

Lovely! So glad to hear this.

PubMed is always a good source. The quality of the work on PubMed varies tremendously, but if you're looking for illustrative examples, it may not matter too much. Google Scholar will find some things that PubMed didn't. If you have a specific topic in mind, searching Cochrane Collaboration can be very helpful.

Potential key words that could point you in the direction of dentally-relevant materials include "tooth", "oral", "dental", "cranio" (or "craniomandibular"), "ortho" (or "orthodontic"), "mineralized tissue" (or its precursor term "bone"), "bruxism", and "temporomandibular" (sometimes abbreviated as "TMJ"--this is a topic that frequently appears in the J. of Oral Rehabilitation). A somewhat less direct path could focus on symptomatology such as "treatment", "pain", "chronic pain", etc. I'm available to help you identify candidate works. Just ask.

Alan

## 2023-12-12

Yes. Let's talk about quasi-experimental designs.

I do have a favor to ask. I want to cite interesting examples from the literature, but I have little experience search for articles relating to dentistry. I can probably figure it out, but if you were doing the searching, would you use PubMed? If so, with what keywords? Are there other places to search that might have a higher probability of finding interesting publications. I'll search directly on the journal's website, of course.

## 2023-12-12

On 12/12/2023 10:34 AM, Glaros, Alan wrote:

Great suggestions, Steve. All of them.

I’d love to go with item 1, but I’m afraid that the audience isn’t at all ready to hear this message. The topics you mentioned in items 2, 2a, and 3 also won’t connect well with this audience. (Alas!)

We’ll have coverage of the issues raised in item 5 via a submission on (dental) practice-based research networks. The idea of “bottom-up science” is repeatedly emphasized.
 
I was very intrigued by the suggestions in item 4 on “quasi-experimental” designs. The idea that randomization isn’t necessarily the best of all possible designs will be news to readers. And describing when to use non-randomized designs (including descriptions of the various design possibilities along with descriptions of their various advantages and disadvantages) would fit in nicely with some other submissions.

So, I love all the ideas, but I think item 4 on quasi-experimental designs is the area in which you could have the greatest impact.

Would this be an acceptable path for you?

## 2023-12-11

Let me write up a few suggestions of topics and see what you think of them. These are roughly in order of my preference, but I can tackle any of these topics. If reviewing these makes you think of a sixth topic that might work better than these, just let me know. I can write a good review paper in a variety of areas.

1. Excessive reliance on p-values in research.

You're aware of the recent ASA statement on p-values. Nice, but a lot of people are unsure what do in practice because we have all become so reliant on p-values. I could talk about weaning yourself away from p-values (as opposed to dropping them cold turkey). There are several places where p-values are clearly unwanted: testing assumptions, especially normality; testing baseline imbalance in randomized studies; using p-values to build models and identify confounders. If we stopped using p-values in places where there is already strong consensus that they are unwanted, that's half the battle already. And it comes relatively pain free.

2. Sample size shortfalls in research.

I can't tell you the number of times that I've had clients come to me asking my permission to stop a study before getting the planned number of subjects. It seems that they had hoped to get a hundred patients within a year, but after two years they have just a couple dozen. This is a bit of irony because if they had exceeded the proposed sample size, they would have to get IRB approval. A change in sample size potentially upsets the cost-benefit balance. But if they fall short of the proposed sample size, they don't have to get IRB approval, even though this also upsets the cost-benefit balance. The big problem is that no one considered monitoring the accrual rate as part of the ongoing management of the trial. If they had caught the slow accrual problem early, they could make adjustments early enough in the trial (e.g., adding extra centers to a multi-center trial, hiring a clinical research coordinator) that it could have an impact. Instead they don't do any thing until the trial is almost done. Then it's too late. There's a Bayesian model for monitoring accrual that I helped develop and it has nice properties for planning prior to data collection, monitoring during data collection, and post study review.

2a. I could write a review of when and why to use a Bayesian approach. While Bayesian methods can apply anywhere, its greatest strength is with hierarchical models, latent models, and imputation of missing values. I'm not a rabid anyone-who-doesn't-use-Bayesian-methods-is-an-idiot Bayesian. I use it when it clearly has advantages over more traditional approaches, but I don't see it being a complete replacement.

3. The proper use of pilot studies in research

Too often, researchers will slap on the label "pilot study" when they have a study that they know is grossly underpowered as a way to deflect criticism of their poor planning. A pilot study is not a miniature version of a full-scale trial. It exists to provide information. It help you estimate a standard deviation to help with sample size justification. It quantifies resource requirements to help plan a budget. It tests the palatability of a new intervention both with the patients and the health care team. It identifies where Murphy's Law is likely to strike. I would also like to address the question of how big a pilot study should be, as there is conflicting advice out in the literature.

4. Quasi-experimental designs

I dislike the term "quasi" because it implies a level of inferiority. You adopt a quasi-experimental approach, in spite of its limitations because it is still superior to an approach like pure randomization. I could review the problems with randomization (heresy, I know, for a statistician to criticize randomization) and then explain some of the more recent approaches to experimental studies where the research team (wisely) rejects randomization because randomization is an inferior approach in a many situations.

5. Community-based participatory research (CBPR)

There's a cynical saying out there about us. It goes something like "Researchers are like mosquitoes; they take your blood and then leave." Many of the problems with research (such as sample size shortfalls and slow patient accrual mentioned above) stem directly from our failure to engage with the communities that we use for research subjects. Using CBPR is very discomforting. It requires you to give up a lot of control, it requires developing listening skills, and it takes a lot of time and energy. But failure to apply CBPR to avoid this discomfort comes at the expense of the quality of your study. One of the most important principles of CBPR is that the community owns the data. I'd like to provide a bit of historical background. Research during the AIDS epidemic was often very confrontational. Advocacy groups made life unpleasant for the research community, but ended up getting many important changes in how we conduct trials today.

## 2023-12-11

Hi Steve,

Here’s the information you requested:

I am guest editing a special issue for the Journal of Oral Rehabilitation. The theme of the special issue is Innovations in Methodology: Promises and Pitfalls. The Journal is well-regarded within dentistry, and is highly-ranked within the field. The goal is to present a "reader's guide" to various topics that will instruct readers and point them in the direction of more technical resources.

Some background: Most of the papers that appear in the Journal of Oral Rehabilitation involve two-group descriptive comparisons, mostly involving groups that differ on some key characteristic. Occasionally, you’ll see a randomized trial.

What you will see very little of (or none at all) are A-B-A designs, N = one-at-a-time designs, adaptive designs, factorial designs, design of experiments, cluster randomized designs, etc. Other papers that I expect to appear in the special issue will discuss “Big Data” approaches to research, the possibilities and perils of AI, and practice-based research networks. (Additional papers will focus on effect sizes, the use of validated outcome measures, psychometrics, and questionnaire construction.) I’m also hoping to identify someone who can write about best practices in the use of experience sampling methods (ecological momentary assessment) or on best practices in the purchase and use of psychophysiological equipment.

As you can tell, I’m hoping there will be an eclectic group of papers in the special issue.

My wish is that a submission on research design will introduce readers to additional methods that expand the repertoire of designs they use in their own investigations. I’ve asked authors to present introductory material that provides basic information, including potential advantages and pitfalls associated with their topics, along with referrals to more authoritative sources for readers who want additional information (the “reader’s guide” component).

Having said all this, I’ve also made it clear that my suggestions are simply that—my suggestions—and that I’m very open to alternative approaches.

The Journal has two broad categories for labeling submissions relevant to the special issue. One is called “original research” and the other is called “review”. I’ve recommended that authors select the “review” category as it offers a larger word limit (7,500 words) and a more flexible organizational scheme. Most of the submissions so far are in the 10-15 page range, not including references, abstract, tables and figures, etc. The new deadline, not yet posted on the journal’s web site is the end of February, 2024.

If you have any questions about any of this, please don’t hesitate to contact me.

Thanks so much for your prompt reply to my LinkedIn message, Steve. I look forward to hearing from you!
