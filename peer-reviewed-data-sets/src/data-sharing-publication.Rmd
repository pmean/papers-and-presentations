---
title: "Early draft of publication"
author: "Adrianna Morse and Steve Simon"
date: "Created: 2020-05-15"
output: html_document
knit: (function(inputFile, encoding) {
  rmarkdown::render(inputFile, encoding = encoding,
  output_dir = "../results", output_format = "all") })  
---

This is an early draft of a paper on data sharing. It was last revised on `r Sys.Date()`.


```{r setup, echo=FALSE}
knitr::opts_chunk$set(echo=FALSE)
suppressMessages(suppressWarnings(library(tidyverse)))
```

## Quality of documentation of data sets found on figshare.

### Introduction

In 2006, Anil Potti led a team of researchers at Duke University and the University of South Florida to develop and publish information about a gene expression profiles that could predict the sensitivity of cancer patients to a broad range of chemotherapeutic drugs (Potti 2006). They used the term "signature" in the title and repeatedly in the paper itself to emphasize that this profile provided a level of individuality comparable to the eccentric scrawl that you use to sign your checks and contracts. Other papers followed and expanded this approach to other drugs.

A pair of Statisticians, Keith Baggerly and Kevin Coombes, reviewed the datasets associated with these publication and found numerous errors (Baggerly 2009). The most telling is for one study where "poor documentation led a report on drug A to include a heatmap for drug B and a gene list for drug C."

This work eventually led to complete invalidation of the model and the withdrawal of the paper. But the problems didn't stop there. Results from these flawed studies led to investment in a start-up company and the initiation of several clinical trials. Several patients have sued Duke University, but the settlement terms are confidential.

https://retractionwatch.com/2015/05/01/malpractice-case-against-duke-anil-potti-settled/

This was not just a "one off" event. Problems with reproducibility have also affected serum proteomic tests of ovarian cancer (Baggerly 2006) 

A careful review of 18 studies published in a journal that required sharing of the raw datasets found that that 8 had serious problems with data (Ioannidis 2009). Either the data was not available, it was changed after publication, it was available only in summary form, or it lacked proper documentation. 

Reproducibility is "the ability of a researcher to duplicate the results of a prior study using the same materials and procedures as were used by the original investigator" (Bollen 2015). Reproducibility is a priority area at the US National Institutes of Health (Collins 2014). It is supported by prominent researchers in Clinical Trials research (Vickers 2006), Computational Science (Davison 2019), Ecology (Powers 2019), Epidemiology (Peng 2006), Social, Behavioral, and Economic Science (Bollen 2015) and many other areas.

Data sharing is a critical component to insuring that research is reproducible. An external assessment of reproducibility is impossible unless the original researchers share their research data with others. The International Committee of Medical Journal Editors, the National Institutes of Health, and the National Science Foundation all have mandates for data sharing. These mandates recognize the importance of patient privacy and the need for the original research team to get their publication out first. Nevertheless, data sharing is expected to be the norm.

Unfortunately, data sharing is hindered by the frequent use of bad storage formats. Researchers often rely on binary formats like Microsoft Word or Adobe PDF that are difficult to import into most statistical software. Another binary format, Microsoft Excel, is easily importable, but suffers from many data quality issues like the conversion of a large number of gene names (Ziemann 2016). Too many data sets use a format of a single proprietary system (such as SPSS or Stata) and lack the interoperability needed for effective data sharing.

Data documentation (when it available at all) can be incomplete. This was even an issue in the data shared by Andrew Vickers in his call for greater data sharing (Vickers 2006). His data set, included with the article, used number codes 0 and 1 for gender without any documentation as to which of these numbers represented females. Other problems, such as failure to specify measurement units are also troublesome.

The goal of this research study is review a convenience sample of data sets stored using Figshare, a popular data sharing site that has been adopted by several prominent journals. Originally, we had planned to reproduce some of the descriptive statistics that appear in the companion peer-reviewed article. That effort, however, was frustrated by the lack of data on Figshare, other than images and summary tables.

### Methods

```{r}
fn <- "../data/asthma-data.txt"
w <- c(11, 3, 3, 5, 3, 3, 2, 2, rep(4, 13))
n <- c(
  "author",
  "da",
  "pr",
  "yyyy",  
  "mm",
  "dd",
  "a",
  "l",
  "csv",
  "doc",
  "eps",
  "mp4",
  "pdf",
  "txt",
  "xls",
  "jpg",
  "tif",
  "py",
  "dx",
  "zip",
  "png" 
)
raw_data <- read.fwf(
  file=fn, 
  widths=w,
  skip=1,
  header=FALSE)
names(raw_data) <- n
```

We ran a pilot study retrieving entries in Figshare associated with the search term "asthma" and posted on Figshare between January 18, 2020 and May 1, 2020.  An entry in Figshare might include more than one data set and a research publication might have more than one Figshare entry. We ended up with 202 data sets in 107 figshare entries corresponding to 56 publications.

The code used to obtain these data sets (note the use of the European format for dates rather than the ISO 8601 standard) was 

:posted_after: 18.01.2020 AND :posted_before: 01.05.2020 AND :search_term: asthma AND :item_type: dataset

The choice of asthma as a search term was arbitrary and reflected an effort to recall a group of entries that was not too heterogenous. The date were chosen to insure at least 100 entries in Figshare (noting that some publications resulted in more than one entry in Figshare).

The figshare resources were downloaded and information was coded on the type of data, the storage format, and the quality of the documentation provided. The peer-review articles were examined to see if they made explicit reference to the data sets stored in figshare. Data were extracted independently by two raters. Any discrepancies were reviewed by the raters and resolved.

The original research plan was to identify four data sets and to try to reproduce just the basic table of descriptive statistics, which might include such things as the mean age and the proportion of female patients.

### Results

We reviewed 107 entries in the Figshare database. This corresponded to 56 separate publications. Some papers had a single entry in Figshare with multiple files in that entry. Others used a separate entry for each file in the database.

More than half of the papers (29) included at least one Microsoft Word document in the repository. The other two commonly used formats were Microsoft Excel (24 papers) and pdf (8 papers). Various image formats (tiff, jpg, png) were used in 9 papers. Surprisingly, the csv format, a very common format for sharing data, was only used by 2 papers.

The percentages appear in Table 1.

The original publications were tracked down for `r sum(raw_data$pr %in% c(" PR", " PW"))` of the papers, but `r sum(raw_data$pr==" PW")` were behind a paywall. There were `r sum(raw_data$pr==" NF")` where the original publication could not be found and `r sum(raw_data$pr==" BL")` where the link was broken.

For the `r sum(raw_data$pr==" PR")` papers that we could review, most (`r sum(raw_data$l==" Y")`) included a link from the paper back to Figshare. There were `r sum(raw_data$l==" O" & raw_data$pr==" PR")` which included a link to a different source than Figshare and `r sum(raw_data$l==" N" & raw_data$pr==" PR")` papers that did not include any links.

Almost all of the papers presented only summary statistics or graphic images in Figshare. For example, the material submitted for Rodriguez (2020) was four Excel spreadsheets that were identical to the four summary tables that appeared in the paper itself. The material submitted for Ohkura (2020) was not a dataset at all, but rather a pair of video files showing digital radiographs of lungs.

One paper asked interested parties to contact the author to get access to the raw data, as there were confidentiality issues that needed to be addressed. There were only `r sum(raw_data$da==" RA")` papers that presented data in unsummarized form, and these files were mostly just summaries of genetic information or in vitro studies. None of the raw datasets included patient level information. The quality of the documentation for the raw datasets was either totally missing, or grossly inadequate. 

1. Abasiyanik (2020) presents the individual results of several laboratory experiments. These experiments have cryptic labels (e.g., SF2a_lig_temp) but the data is documented well enough to see that the individual conditions were run in triplicate. The conditions that were run appear to be labeled reasonably well, but we could not find the paper associated with these data sets to confirm this.

2. The CSV file in Fischer (2020) used semicolons as delimiters instead of commas. The column names (Number, SMILES, Ki(M1) [nM], Ki(M2) [nM], Ki(M3) [nM]) did include unit of measurement but were otherwise difficult to follow.

3. Gal (2020) presented the results of several gene assays.

4. Guo (2020)

5. Maghsoudloo (2020)

6. The spreadsheet in Mak (2020) had a label at the top of each column, but the names were cryptic and there was no data dictionary. The Excel file did include a "readme" sheet but the entries were unhelpful to the general reader.
  File is in EPACT format	
  MARKER_ID	chr:pos_REF/ALT
  Statistics reported relative to ALT allele	

7. Sordillo (2020)

8. Zimova (2020)

```{r}
nice_percent <- function(lab, df=raw_data) {
  # Count and display non-zero count and percentage
  x <- df[ , lab]
  num <- sum(x!=0)
  den <- length(x)
  pct <- paste0(round(100*num/den), "%")
  n <- paste0("(", num, ")")
  paste(lab, pct, n)
}
tab <- NULL
tab <- append(tab, nice_percent("doc"))
tab <- append(tab, nice_percent("xls"))
tab <- append(tab, nice_percent("pdf"))
tab <- append(tab, nice_percent("tif"))
tab <- append(tab, nice_percent("jpg"))
tab <- append(tab, nice_percent("csv"))
```

#### Table 1. Formats used

`r paste(tab, collapse="\n\n")`

Note: The formats dx, eps, mp4, png, py, txt, and zip were used once.


### Discussion

The results of this pilot study were extremely disappointing and highlight some major weaknesses of data sharing systems that are likely to be an issue beyond this particular data sharing system.

There were troublesome inconsistencies in the use of Figshare entries. Some authors registered each separate data file as an individual entry in Figshare. Others placed all the files in a single Figshare entry.

Standards for how to share data are largely lacking, except in certain specialized areas (use MIAME as an example).

What standards do exist are not enforced by Figshare. The Figshare software is designed to be highly flexible to make it easy for researchers to submit data to their repository, which is an admirable goal. This flexibility, however, comes at the expense of the persons trying to find and use datasets stored on Figshare.

There are some positive features to note about Figshare. First, it assigns a unique digital object identifier (DOI) to each dataset. The DOI is a persistent address on the web that insures that a web page does not disappear when the original URL changes because of a re-organization of a directory structure or a change in domain names.


### References

```{r}
save.image("../data/data-sharing-publication.RData")
```